import streamlit as st
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import altair as alt
# streamlit_app.py

import hmac
# st.set_page_config(layout="wide")

def check_password():
    """Returns `True` if the user had the correct password."""

    def password_entered():
        """Checks whether a password entered by the user is correct."""
        if hmac.compare_digest(st.session_state["password"], st.secrets["password"]):
            st.session_state["password_correct"] = True
            del st.session_state["password"]  # Don't store the password.
        else:
            st.session_state["password_correct"] = False

    # Return True if the password is validated.
    if st.session_state.get("password_correct", False):
        return True

    # Show input for password.
    st.text_input(
        "Password", type="password", on_change=password_entered, key="password"
    )
    if "password_correct" in st.session_state:
        st.error("üòï Password incorrect")
    return False


if not check_password():
    st.stop()  # Do not continue if check_password is not True.

# Descrever o projeto brevemente
col1, col2 = st.columns([8, 1])
with col1:
  st.write("# Gr√£o Direto IA Challenge")
with col2:
  st.image('images/gd2.png', width=200)

with st.expander("An√°lise explorat√≥ria e abordagens iniciais", False):
  """
  #### O desafio consiste em criar um modelo de Machine Learning para prever a probabilidade de um cliente realizar uma transa√ß√£o.
  #### Para isso, buscaremos entender um pouco melhor os dados visualmente.

  **Entendendo a varia√ß√£o do CBOT, pre√ßo de mercado e D√≥lar no tempo, ambos a n√≠vel de produto.**"""

  img1 = plt.imread('images/precosnomtepo.png')
  st.image(img1, caption='Pre√ßo dos gr√£os e dolar pelo tempo (informa√ß√µes de mercado)')

  "O gr√°fico acima j√° se mostrou mais informativo, de modo que √© percept√≠vel que as tend√™ncias do `Milho` e da `Soja` s√£o extremamente parecidas em gr√°ficos temporais, muito embora a correla√ß√£o entre `CBOT` e `PRICE` quando comparados os gr√£os n√£o tenha se mostrado alta."

  "Agora vamos analisar a varia√ß√£o do `price` e do `amount` ao longo do tempo, para tentar entender melhor a rela√ß√£o entre essas vari√°veis."

  "**Vamos tamb√©m dar uma olhada a a varia√ß√£o do pre√ßo e compra/venda de gr√£os no tempo**"

  img2 = plt.imread('images/transacoes_no_tempo.png')
  st.image(img2, caption='Pre√ßo pago e n√∫mero de sacas no tempo com o d√≥lar (informa√ß√µes de mercado)')

  """Aparentemente, apresenta-se uma mesma tend√™ncia para o `milho` e a `soja`, no entanto foi observado um outlier extremo para a soja no m√™s de Maio.

  Esse outlier pode ser potencialmente algum erro nos dados, mas n√£o pode ser descartado que o mercado possa ter oscilado muito. A op√ß√£o ser√° por mant√™-lo.

  Caso o modelo apresente resultados ruins, podemos retornar e fazer a remo√ß√£o do mesmo."""


  "**Vamos tentar entender melhor a diferen√ßa entre o `Milho` e a `Soja`.**"

  img3 = plt.imread('images/milho_soja.png')
  st.image(img3, caption='Pre√ßo dos gr√£os e dolar pelo tempo')

  "Parece haver duas nuvens separadas de pontos, o que sugere dizer que essas nuvens separadas se d√£o pela diferen√ßa de gr√£os."

  """**Vamos reunir as informa√ß√µes de mercado e transa√ß√µes por m√™s e buscar potenciais correla√ß√µes:**"""
  img4 = plt.imread('images/heatmap_.png')
  st.image(img4, caption='Heatmap de correla√ß√£o entre as vari√°veis')

  """- `D√≥lar` e `CBOT` t√™m forte impacto nas vari√°veis relacionadas a pre√ßo e quantidade.
  - Pre√ßo de mercado e pre√ßo de transa√ß√µes s√£o altamente correlacionados, o que √© esperado."""

  """**Buscando entender tamb√©m as vari√°veis `categ√≥ricas`**"""

  img5 = plt.imread('images/cat_mercado.png')
  st.image(img5, caption='Vari√°veis categ√≥ricas de mercado')

  img5 = plt.imread('images/cat_transacoes.png')
  st.image(img5, caption='Vari√°veis categ√≥ricas de transa√ß√µes')

  """
  `MG, MT e RO` se mostraram como os estado que mais tiveram exporta√ß√µes de `soja` (*`MG - tem a ver com a gr√£o direto?!`* haha).

  Ao que parece, o `MT` tamb√©m apresenta influencia relevante nas vendas de `Milho`.

  A companhia `Polaris` tamb√©m se mostrou como forte exportadora de `soja`, potencialmente como a principal empresa de exporta√ß√£o tanto da soja quanto dos gr√£os em geral.

  O `milho` parece n√£o ser muito vendido, e provavelmente a compra da soja seja mais atrativa para os compradores.

  Vamos tentar entender um pouco melhor sobre os vendedores, que parece ser uma vari√°vel extremamente importante para a an√°lise atual.
  """
  img5 = plt.imread('images/vendedores.png')
  st.image(img5, caption='Top 10 vendedores em todo o per√≠odo')

  "Parecem existir vendedores que atuam muito mais fortemente no mercado, e esses 10 representam `11% do total de vendedores`."

with st.expander("Abordagem para a modelagem", False):
  """# Abordagens para a modelagem

  Ser√£o removidas as colunas:
  
  - `Price (transa√ß√µes)`
  - `Amount (transa√ß√µes)`

  **Pois s√£o colunas dependentes de que haja uma transa√ß√£o, e o que queremos descobrir √© se haver√° uma transa√ß√£o, ent√£o n√£o faz sentido mant√™-las, al√©m de potencialmente causar vazamento de dados.**

  Adi√ß√£o das colunas:

  - `Dia do m√™s`
  - `Dia da semana`
  - `Semana do m√™s`
  """

with st.expander("Ponto important√≠ssimo sobre a modelagem", False):
   """Percebe-se que os dados de 2024-11-05 j√° foram dados, que s√£o:
   - `CBOT`,
   - `Pre√ßo de mercado`,
   - `D√≥lar (obtido no c√≥digo)`.

   
   **No entanto, em um caso real, `esses dados n√£o estariam dispon√≠veis`, e para obt√™-los, precisar√≠amos prev√™-los com modelos de regress√£o, tamb√©m baseados em s√©ries temporais (ou t√©cnicas mais avan√ßadas)**.
   
   Em posse desses dados preditos, poder√≠amos alimentar o modelo presente no projeto atual.

   **Perceba que em um mercado de gr√£os, tais como em mercados de a√ß√µes, h√° muitas incertezas e portanto a predi√ß√£o do modelo aqui presente sendo alimentada por predi√ß√µes de regress√£o de outros modelos potencialmente adicionaria mais incertezas ao modelo final.**
   """

with st.expander("Abordagem para a predi√ß√£o", False):
  """
  ## Abordagem para a predi√ß√£o
  **A op√ß√£o ser√° por utilizar o Vendedor `(Seller ID)` como vari√°vel alvo e APENAS os `dados de mercado, aliado √†s vari√°veis mencionadas acima` colunas como vari√°veis independentes.**

  **A m√©trica de avalia√ß√£o principal ser√° o `precision_weighted`, pois nos importa mais se o que o modelo diz est√° correto, buscando evitar Falsos positivos do modelo, pois j√° ter√≠amos mais certeza do pr√≥ximo dia, nos antecipando √†s demandas de mercado e executando as opera√ß√µes e transa√ß√µes corretas.**

  A abordagem √© pelo precision `weighted`, pois essa vari√°vel nos proporciona uma m√©dia ponderada do precision de cada classe, o que √© importante para a an√°lise, uma vez que temos dados desbalanceados e um n√∫mero muito grande de classes.

  A abordagem ser√° de utilizar modelos de Machine Learning Baseados em √°rvores de decis√£o, principalmente adaptados a s√©ries temporais, e os motivos s√£o:
  - Modelos de √°rvore proporcionam probabilidade de classifica√ß√£o
  - Boa capacidade preditiva para multiclasses (n√∫mero de vendedores)
  - Bom funcionamento com vari√°veis desbalanceadas.
  """

with st.expander("Modelos utilizados", False):
  """Modelos utilizados nas an√°lises:
  - Random Forest,
  - Extra trees,
  - √Årvores de decis√£o,
  - Dummy Classifier.

  **E tamb√©m modelos de Machine Learning adaptados a s√©ries temporais (consultar documenta√ß√£o em [sktime](https://www.sktime.net/en/stable/)), como:**
  - `TimeSeriesForestClassifier`
  """

  """### Observa√ß√£o, todos os testes foram feitos com `Time Series Split validation` [sklearn-TimeSeriesSplit](https://scikit-learn.org/dev/modules/generated/sklearn.model_selection.TimeSeriesSplit.html), respeitando a ordem temporal dos dados."""

with st.expander("Tentativas de modelagem", False):
  """Foram tentadas diversas abordagens para a modelagem, desde:
  - Remover vendedores com menos de 5 vendas e com vendas ausentes nos √∫ltimos 30 dias,
  - Remover apenas vendedores com menos de 5 vendas,
  - Utilizar todo o conjunto de vendedores.

  **Utilizar `todo o conjunto de vendedores` foi o que nos apresentou melhores resultados, e por isso foi a abordagem escolhida para a modelagem final.**
  """

with st.expander("Resultados da modelagem", False):
  """O modelo que apresentou o melhor resultado nos dados de teste foi o `TimeSeriesForestClassifier`, com um `precision_weighted` de aproximadamente `10%` ***(Note que embora o valor inicialmente pare√ßa baixo, trata-se de uma predi√ß√£o multiclasse com 2423 diferentes vendedores)***."""

  """
  Para a predi√ß√£o do dia `11-04-2024`, os sellers IDs cujas probabilides foram maiores podem ser vistos pela tabela a seguir:
  """

df_probs_14 = pd.read_csv('top_sellers_04_11.csv')
df_probs_15 = pd.read_csv('probabilities_15.csv')
df_probs_15['Probability'] = round(df_probs_15['Probability'].mul(100),2)
df_probs_14['Probability'] = round(df_probs_14['Probability'].mul(100),2)
df_probs_14.rename({'Probability':'Probability (%)'}, axis=1,inplace=True)
df_probs_15.rename({'Probability':'Probability (%)'}, axis=1, inplace=True)

# Add a slider to select the number of top sellers to display



with st.expander("Probabilidades de transa√ß√µes dos vendedores", False):
  num_sellers = st.slider('Selecione o n√∫mero m√°ximo de vendedores com maior probabilidade de venda para o dia 14-11:', min_value=1, max_value=100, value=10)
  top_sellers = df_probs_14.head(num_sellers)
  st.write(top_sellers)


  f"""Essa predi√ß√£o foi feita pra cada linha do conjunto de teste. No caso, temos {df_probs_14.shape[0]} vendedores que poderiam fazer uma transa√ß√£o no dia 04/11/2024, pois para cada informa√ß√£o de mercado, h√° a probabilidade de diferentes vendedores realizarem a transa√ß√£o. """

  # Add a slider to select the number of top sellers to display
  num_sellers = st.slider('Selecione o n√∫mero m√°ximo de vendedores com maior probabilidade de venda para o dia 15-11:', min_value=1, max_value=100, value=10)

  # Display the top sellers based on the selected number
  top_sellers = df_probs_15.head(num_sellers)
  st.write(top_sellers)


  df_results = pd.read_csv('df_results.csv')
  df_results.rename(columns={'Unnamed: 0': 'Vari√°vel'}, inplace=True)
  df_sorted = df_results.sort_values(by="mean", ascending=False)
  df_sorted = df_sorted.set_index('Vari√°vel')

  """E aqui pode-se analisar o desempenho de cada m√©trica para o modelo"""
  # Criar um gr√°fico de barra com Altair
  chart = alt.Chart(df_sorted.reset_index()).mark_bar(color='#008000').encode(
    x='mean:Q',
    y=alt.Y('Vari√°vel:N', sort='-x')
  ).properties(
    height=500
  )

with st.expander("Import√¢ncia das vari√°veis no modelo", False):
  st.altair_chart(chart, use_container_width=True)

  """Aqui podemos ver as features que mais impactaram na tomada de decis√£o do modelo (as features que mais reduzem a imperza nos n√≥s das √°rvores do modelo).

  Reiterando que essas features s√£o as que mais impactaram na decis√£o do modelo, e n√£o s√£o necessariamente as features que efetivamente determinam o mercado, mas a rela√ß√£o entre os dados que o modelo encontrou foram essas:

  - `Dolar`,
  - `Origin state`,
  - `Origin city`,
  - `Price (mercado)`,
  - `CBOT`.

  Aparentemente o d√≥lar foi uma √≥tima adi√ß√£o ao modelo, sendo uma das features que mais influenciam e proporcionam informa√ß√£o pro mesmo fazer a predi√ß√£o."""


with st.expander("Conclus√µes", False):
  """
  # Conclus√£o e Pr√≥ximos passos

  O projeto atual se mostrou bastante desafiador por tr√™s principais motivos: A predi√ß√£o de s√©ries temporais para classifica√ß√£o, com um n√∫mero muito grande de classes e que requisitasse probabilidade. Al√©m da limita√ß√£o de n√£o poder utilizarmos modelos de redes neurais.

  O modelo feito n√£o teve um prediction muito alto, dada a complexidade da predi√ß√£o de diversas classes e a quantidade de dados dispon√≠veis.
  No entanto, alguns pontos poderiam ter sido utilizados de modo a melhorar o modelo:

  - Utilizar modelos de Deep learning, que teriam um potencial exponencialmente maior de capturar padr√µes nos dados.
  - Tentar mais t√©cnicas de feature engineering, trazendo dados de neg√≥cio, como fontes do IBGE, CONAB, e outras fontes de dados do mercado de gr√£os.
  - Realizar um tuning de hiperpar√¢metros mais extenso, de modo a descobrir features melhores dos modelos e reduzir o overfitting, aumentando a capacidade de generanaliza√ß√£o do modelo.
  - Potencialmente fazer convers√µes nos dados para buscar uma regress√£o (pensei nesta abordagem durante o projeto, mas n√£o encontrei alguma t√©cnica plaus√≠vel).
  - Com mais tempo, fazer uma an√°lise explorat√≥ria mais profunda, buscando encontrar padr√µes potencialmente ainda n√£o encontrados na atual an√°lise.
  - Transformar o modelo numa abordagem bin√°ria, onde o Seller ID poderia ser uma categoria (eu fiz essa abordagem, mas isso tornou o dataset muito grande e fiquei sem recursos para o modelo).
  - Com a presen√ßa de dados reais, poder√≠amos ter mais insumo pra alimentar o modelo, tamb√©m potencialmente contribuindo com a redu√ßao de overfitting.
  - Testar outros modelos do sktime para predi√ß√£o de s√©ries temporais.
  - Fazer convers√µes nos dados de modo a utilizar abordagem comum de s√©ries temporais (ARIMA, SARIMA, Seasonal decomposing, suaviza√ßao exponencial).
  - Fazer o LAG nos dados, oo que potencialmente aumentaria a capacidade de predi√ß√£o de modelos de machine learning com abordagem tradicional.
  - Fazer o agrupamento dos dados com KMeans, de modo a reduzir o n√∫mero de classes e fazer a predi√ß√£o sobre um grupo de potenciais vendedores (aprendizagem semi supervisionada).
  - Fazer uma abordagem de classifica√ß√£o bin√°ria, onde cada seller_id poderia ser um imput e a predi√ß√£o seria se ele faria ou n√£o uma transa√ß√£o, no entanto ap√≥s tentar realizar essa abordagem, percebeu-se dificuldade em lidar com um grande dataset gerado.

  Enfim... V√°rias s√£o as ideias, muitas delas n√£o houve tempo ou recursos computacionais para serem executadas, mas no geral acredito que a abordagem tenha seguido bons padr√µes e atendido √† demanda que era a de entregar a probabilidade de potenciais compradores nos dias 04/11 e 05/11/2024.

  """